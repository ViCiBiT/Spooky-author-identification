{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "train1 = pd.read_csv(r\"C:\\Users\\Admin\\Documents\\GitHub\\Spooky-author-identification\\train.csv\")#19579\n",
    "test = pd.read_csv(r\"C:\\Users\\Admin\\Documents\\GitHub\\Spooky-author-identification\\test.csv\")#8392\n",
    "submit = pd.read_csv(r\"C:\\Users\\Admin\\Documents\\GitHub\\Spooky-author-identification\\sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import floor\n",
    "valid= train1.loc[:floor((train1.shape[0]*1)/3)]\n",
    "train = train1.loc[floor((train1.shape[0]*1)/3):]\n",
    "#train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13053\n"
     ]
    }
   ],
   "source": [
    "print train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#tokenize text\\ndef token(doc):\\n    split = \" \".join(i for i in doc.lower().split())\\n    words = re.findall(r\"[\\\\w\\']+\", split)\\n    stop_free = \" \".join([i for i in words if i not in stop]) # no stop word\\n    punc_free = \\'\\'.join(ch for ch in stop_free if ch not in exclude) # no punctuation \\n    token_a = re.findall(r\"[\\\\w\\']+\", punc_free)\\n    token_b =[ps.stem(a) for a in token_a]\\n    token_c = \" \".join([b for b in token_b])\\n    return token_c\\ntrain[\"token\"] = [token(a) for a in train[\"text\"]]\\nvalid[\"token\"]= [token(a) for a in valid[\"text\"]]\\ntest[\"token\"] = [token(a) for a in test[\"text\"]]'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "stop = set(stopwords.words('english'))# such as a, an, and, are,\n",
    "exclude = set(string.punctuation) # such as: , . \"\" {}\n",
    "\"\"\"\n",
    "#tokenize text\n",
    "def token(doc):\n",
    "    split = \" \".join(i for i in doc.lower().split())\n",
    "    words = re.findall(r\"[\\w']+\", split)\n",
    "    stop_free = \" \".join([i for i in words if i not in stop]) # no stop word\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude) # no punctuation \n",
    "    token_a = re.findall(r\"[\\w']+\", punc_free)\n",
    "    token_b =[ps.stem(a) for a in token_a]\n",
    "    token_c = \" \".join([b for b in token_b])\n",
    "    return token_c\n",
    "train[\"token\"] = [token(a) for a in train[\"text\"]]\n",
    "valid[\"token\"]= [token(a) for a in valid[\"text\"]]\n",
    "test[\"token\"] = [token(a) for a in test[\"text\"]]\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dividing the training set into groups\n",
    "#hpl = train[train[\"author\"] == \"HPL\"][['id','token']]\n",
    "#mws = train[train[\"author\"] == \"MWS\"][['id','token']]\n",
    "#eap = train[train[\"author\"] == \"eap\"][['id','token']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Naive Bayesian Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TFIDF model\n",
    "tfidf = TfidfTransformer()\n",
    "#vector count model\n",
    "vector= CountVectorizer(decode_error = 'ignore',stop_words = 'english')\n",
    "# fit and transform the training set\n",
    "X_train = vector.fit_transform(list(train['text']))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "y_train = list(train['author'])\n",
    "#transform the testing set with the training model\n",
    "X_test = vector.transform(list(valid['text']))\n",
    "X_test_tfidf= tfidf.transform(X_test)\n",
    "y_test = list(valid['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13053, 21147)\n",
      "13053\n",
      "(6527, 21147)\n",
      "6527\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "print len(y_train)\n",
    "print X_test.shape\n",
    "print len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.821357438333078"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted = clf.predict(X_test_tfidf)\n",
    "float(np.mean(predicted == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()), ('clf_svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter=5, random_state=42))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7464378734487513"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_= text_clf_svm.fit(list(train['text']), list(train['author']))\n",
    "svm_predicted = text_clf_svm.predict(list(valid['text']))\n",
    "float(np.mean(svm_predicted == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
