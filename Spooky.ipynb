{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preprocessing\n",
    "- Neural network\n",
    "- XGboost??\n",
    "- SVM tuning parameter\n",
    "- Decision tree\n",
    "- Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named lime",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1c51538d171b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mlime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named lime"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from math import floor\n",
    "stopword = set(stopwords.words('english'))\n",
    "porter = PorterStemmer()\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import lime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To preprocess, we solve encoding problem and cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'awn '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'awn Ã¢'.decode('utf-8').encode('ascii','ignore')\n",
    "#decode: from string to unicode\n",
    "#encode: from unicode to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = text.decode('utf-8').encode('ascii', 'ignore')\n",
    "    words = word_tokenize(text) #split words\n",
    "    words = [w.lower() for w in words if w.isalpha()] #get rid of punctuation\n",
    "    words =[w for w in words if  not w in stopword]\n",
    "    stemmed = [porter.stem(w) for w in words]\n",
    "    stemmed = ' '.join(w for w in stemmed)\n",
    "    return stemmed\n",
    "\n",
    "def create_dataset():\n",
    "    data = {}\n",
    "    train = pd.read_csv(r\"train.csv\")#19579\n",
    "    test = pd.read_csv(r\"test.csv\")#8392\n",
    "    data[\"submit\"] = pd.read_csv(r\"sample_submission.csv\")\n",
    "    \n",
    "    train['Token'] = train.text.map(tokenize)\n",
    "    test['Token'] = test.text.map(tokenize)\n",
    "    \n",
    "    data['valid'] = train.loc[:floor((train1.shape[0]*1)/3)]\n",
    "    data['train'] = train.loc[floor((train1.shape[0]*1)/3):]\n",
    "    data['test'] = test\n",
    "    \n",
    "    data['valid'] = data['valid'].set_index('id')\n",
    "    data['test'] = data['test'].set_index('id')\n",
    "    data['train'] = data['train'].set_index('id')\n",
    "    \n",
    "    return data\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>Token</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id23567</th>\n",
       "      <td>He was the devil incarnate, Birch, and I belie...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>devil incarn birch believ eye eye furi could b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id26623</th>\n",
       "      <td>We are ready to expose our breasts, exposed te...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>readi expos breast expos ten thousand time bal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id17046</th>\n",
       "      <td>He continued, \"You must create a female for me...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>continu must creat femal live interchang sympa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id26351</th>\n",
       "      <td>And always the goal of my fancies was the migh...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>alway goal fanci mighti vine grown wall littl ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id13279</th>\n",
       "      <td>While I remained motionless, and busied in end...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>remain motionless busi endeavor collect though...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text author  \\\n",
       "id                                                                  \n",
       "id23567  He was the devil incarnate, Birch, and I belie...    HPL   \n",
       "id26623  We are ready to expose our breasts, exposed te...    MWS   \n",
       "id17046  He continued, \"You must create a female for me...    MWS   \n",
       "id26351  And always the goal of my fancies was the migh...    HPL   \n",
       "id13279  While I remained motionless, and busied in end...    EAP   \n",
       "\n",
       "                                                     Token  \n",
       "id                                                          \n",
       "id23567  devil incarn birch believ eye eye furi could b...  \n",
       "id26623  readi expos breast expos ten thousand time bal...  \n",
       "id17046  continu must creat femal live interchang sympa...  \n",
       "id26351  alway goal fanci mighti vine grown wall littl ...  \n",
       "id13279  remain motionless busi endeavor collect though...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13053, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = ['EAP', 'HPL','MWS']\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dividing the training set into groups\n",
    "#hpl = train[train[\"author\"] == \"HPL\"][['id','token']]\n",
    "#mws = train[train[\"author\"] == \"MWS\"][['id','token']]\n",
    "#eap = train[train[\"author\"] == \"eap\"][['id','token']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for training process, we turn each documnet into vector of tfidf. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* vector.vocabulary_ : a dictionary with keys are list of terms while the values are the id of the according word (e.g: 'fawn': 4296) \n",
    "* X_train: a matrix shaper 13053 x 13304 ( # of doc * # of terms), can use X_train[0,:].toarray() to print out each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_valid = vector.transform(list(valid['token']))\n",
    "X_valid_tfidf= tfidf.transform(X_test)\n",
    "y_valid = list(valid['author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Naive Bayesian Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TFIDF model\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "#vector count model\n",
    "vector= CountVectorizer(decode_error = 'ignore',stop_words = 'english')\n",
    "# fit and transform the training set\n",
    "X_train = vector.fit_transform(list(train['Token']))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "y_train = list(train['author'])\n",
    "\n",
    "#transform the testing set with the training model\n",
    "X_test = vector.transform(list(valid['Token']))\n",
    "X_test_tfidf= tfidf.transform(X_test)\n",
    "y_test = list(valid['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13053, 13304)\n",
      "13053\n",
      "(6527, 13304)\n",
      "6527\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "print len(y_train)\n",
    "print X_test.shape\n",
    "print len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8153822583116286"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted = clf.predict(X_test_tfidf)\n",
    "float(np.mean(predicted == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four kernels in SVM: linear kernel, polynomial kernel, RBF(gaussian) kernel, string kernel.For text classification, linear kernel is recommended because:\n",
    "\n",
    "- Text classification problem is oftern linearly separable\n",
    "- Large number of feature so mapping a larger space doesn't improve performance \n",
    "- Faster\n",
    "- Less parameters to optimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#try linear and rbf kernels with different value of C and gamma\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1,4,7,10], 'gamma': \n",
    "              [0.01,0.04,0.10,0.5]}\n",
    "svr = svm.SVC()\n",
    "clf_svm1 = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()), ('grid',  GridSearchCV(svr, parameters))])\n",
    "_ = clf_svm1.fit(list(train['Token']), list(train['author']))\n",
    "svm_predicted = clf_svm1.predict(list(valid['Token']))\n",
    "float(np.mean(svm_predicted == y_test))\n",
    "#C = 1: 0.79\n",
    "#c= 0.5: 0.791 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_test, predicted)\n",
    "print(cnf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_clf_svm = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()), ('clf_svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter = 10000, random_state=42))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_= text_clf_svm.fit(list(train['Token']), list(train['author']))\n",
    "svm_predicted = text_clf_svm.predict(list(valid['Token']))\n",
    "float(np.mean(svm_predicted == y_test))\n",
    "#no preprocess: 0.746\n",
    "#preprocess: 0.758"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Print out the test result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test['Token'] = test.text.map(tokenize)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "_ = clf.fit(list(train1['Token']), list(train1['author']))\n",
    "svm_predicted = clf.predict_proba(list(test['Token']))\n",
    "svm_predicted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eap = [a[0] for a in svm_predicted]\n",
    "hpl = [a[1] for a in svm_predicted]\n",
    "mws = [a[2] for a in svm_predicted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "items = [('id', list(test['id'])), ('EAP',eap),('HPL',hpl),('MWS',mws)]\n",
    "file_test = pd.DataFrame.from_items(items)\n",
    "file_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_test.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_test.to_csv('result.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#may think of clean the text first(tokenize), try with SVM next time"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
